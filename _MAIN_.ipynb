{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEKDEcZY0Ed9"
      },
      "source": [
        "# **Machine learning for low-resource NLP**: Advancing AI for Linguistic Inclusion\n",
        "Cross-lingual transfer learning and pseudo-labeling for multilingual named entity recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wu7-PcR2xzI"
      },
      "source": [
        "**General Imports:** Import fundamental libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMkZmRoX2xLR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizerFast\n",
        "from config import BaseConfig\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**User-defined Imports:** Import custom classes and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model import BertBilstmCrf\n",
        "from train import train_model, evaluate_epoch\n",
        "from utils.dataloader import create_dataloaders\n",
        "from pseudo_labeling import train_pseudo_labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Configs:** Constants for model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class BaseConfig:\n",
        "    # Miscelanous\n",
        "    RANDOM_STATE          = 42\n",
        "    DEVICE                = torch.device(\"cuda\")\n",
        "\n",
        "    # Data\n",
        "    low_resource_langs    = [\"mg\", \"fo\", \"co\", \"hsb\", \"bh\", \"cv\"]\n",
        "    high_resource_langs   = [\"id\", \"da\", \"it\", \"pl\", \"hi\", \"tr\"]\n",
        "\n",
        "    NUM_TAGS              = 7\n",
        "    BATCH_SIZE            = 32\n",
        "    MAX_SEQ_LEN           = 80\n",
        "\n",
        "class TrainConfig(BaseConfig):\n",
        "    EPOCHS                = 20\n",
        "    PATIENCE              = 5\n",
        "    BERT_LEARNING_RATE    = 0.00003\n",
        "    LSTM_LEARNING_RATE    = 0.005\n",
        "    CRF_LEARNING_RATE     = 0.00005\n",
        "    WEIGHT_DECAY          = 0.02\n",
        "\n",
        "class FineTuneConfig(BaseConfig):\n",
        "    EPOCHS                = 15\n",
        "    PATIENCE              = 3\n",
        "    BERT_LEARNING_RATE    = 0.00002\n",
        "    LSTM_LEARNING_RATE    = 0.003\n",
        "    CRF_LEARNING_RATE     = 0.00003\n",
        "\n",
        "class PseudoLabelingConfig(BaseConfig):\n",
        "    EPOCHS                = 25\n",
        "    PATIENCE              = 5\n",
        "    BERT_LEARNING_RATE    = 0.00002\n",
        "    LSTM_LEARNING_RATE    = 0.003\n",
        "    CRF_LEARNING_RATE     = 0.00003\n",
        "\n",
        "    CONFIDENCE_QUANTILE   = 0.965\n",
        "    PSEUDO_DELAY          = 8\n",
        "    ENTROPY_THRESHOLD     = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5TPSU9HgdWq"
      },
      "source": [
        "**Set Random Seed:** Ensure random seeds are all set to esnure reproducibility of results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4KsqUJuTgpSX"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(TrainConfig.RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_7UwDji3Cfo"
      },
      "source": [
        "**Data Processing:** Load WikiANN data from HuggingFace and split into train/val/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "83jA6h2-zZiy",
        "outputId": "c5b149b6-ec47-41da-ee95-c5980771a869"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_wikiann_datasets(language_codes, cutoff=None):\n",
        "\n",
        "    language_data = {}\n",
        "    for lang in language_codes:\n",
        "        \n",
        "        # Load raw data from hugging face\n",
        "        lang_dataset = load_dataset(\"unimelb-nlp/wikiann\", name=lang)\n",
        "\n",
        "        # Get data from different splits and combine\n",
        "        train_df = pd.DataFrame(lang_dataset[\"train\"])\n",
        "        val_df = pd.DataFrame(lang_dataset[\"validation\"])\n",
        "        test_df = pd.DataFrame(lang_dataset[\"test\"])\n",
        "\n",
        "        complete_df = pd.concat([train_df, val_df, test_df]).reset_index(drop=True)\n",
        "        complete_df = complete_df.head(cutoff) if cutoff else complete_df\n",
        "\n",
        "        # Split data into new train/val/test splits\n",
        "        train, temp = train_test_split(complete_df, test_size=0.2, random_state=TrainConfig.RANDOM_STATE)\n",
        "        val, test = train_test_split(temp, test_size=0.5, random_state=TrainConfig.RANDOM_STATE)\n",
        "\n",
        "        language_data[lang] = {\"train\": train, \"val\": val, \"test\": test}\n",
        "\n",
        "    return language_data\n",
        "\n",
        "# Download and store data\n",
        "low_resource_datasets = load_wikiann_datasets(TrainConfig.low_resource_langs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**NER Dataset:** Create dataset for Wikiann NER data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NERDataset:\n",
        "    def __init__(self, texts, tags, include_sentence = False):\n",
        "        self.texts = texts\n",
        "        self.tags = tags\n",
        "\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(\n",
        "            \"google-bert/bert-base-multilingual-cased\", do_lower_case = True\n",
        "        )\n",
        "\n",
        "        self.CLS_TOKEN = [101]\n",
        "        self.SEP_TOKEN = [102]\n",
        "        self.PAD_TOKEN = [0]\n",
        "        self.MAX_LEN = BaseConfig.MAX_SEQ_LEN\n",
        "\n",
        "        # Determines if the original sentence is returned for each batch\n",
        "        self.include_sentence = include_sentence\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        tags = self.tags[index]\n",
        "\n",
        "        token_ids = []\n",
        "        target_tags = []\n",
        "        for i, word in enumerate(text):\n",
        "            word_ids = self.tokenizer.encode(word, add_special_tokens = False)\n",
        "            token_ids.extend(word_ids)\n",
        "            target_tags.extend(len(word_ids) * [tags[i]])\n",
        "\n",
        "        # Resize for special tokens\n",
        "        token_ids = token_ids[:self.MAX_LEN - 2]\n",
        "        target_tags = target_tags[:self.MAX_LEN - 2]\n",
        "\n",
        "        # Add special tokens\n",
        "        token_ids = self.CLS_TOKEN + token_ids + self.SEP_TOKEN\n",
        "        target_tags = self.PAD_TOKEN + target_tags + self.PAD_TOKEN\n",
        "\n",
        "        attention_mask = [1] * len(token_ids)\n",
        "        token_type_ids = [0] * len(token_ids)\n",
        "\n",
        "        # Add padding to make sure all inputs are the same size\n",
        "        padding_len = self.MAX_LEN - len(token_ids)\n",
        "        token_ids += [0] * padding_len\n",
        "        target_tags += [0] * padding_len\n",
        "        attention_mask += [0] * padding_len\n",
        "        token_type_ids += [0] * padding_len\n",
        "\n",
        "        if self.include_sentence:\n",
        "            return {\n",
        "                \"input_ids\": torch.tensor(token_ids, dtype = torch.long),\n",
        "                \"target_tags\": torch.tensor(target_tags, dtype = torch.long),\n",
        "                \"attention_mask\": torch.tensor(attention_mask, dtype = torch.long),\n",
        "                \"token_type_ids\": torch.tensor(token_type_ids, dtype = torch.long),\n",
        "                \"orginal_text\": \" \".join(text)\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(token_ids, dtype = torch.long),\n",
        "            \"target_tags\": torch.tensor(target_tags, dtype = torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype = torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype = torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Dataloaders:** Define functions for creating dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataloader(lang_split_data, CONFIG, include_sentence=False):\n",
        "    dataset = NERDataset(\n",
        "        lang_split_data[\"tokens\"].to_list(),\n",
        "        lang_split_data[\"ner_tags\"].to_list(),\n",
        "        include_sentence = include_sentence\n",
        "    )\n",
        "    return DataLoader(dataset, CONFIG.BATCH_SIZE)\n",
        "\n",
        "def create_dataloaders(lang_data, CONFIG):\n",
        "\n",
        "    train_loader = create_dataloader(lang_data[\"train\"], CONFIG)\n",
        "    val_loader = create_dataloader(lang_data[\"val\"], CONFIG)\n",
        "    test_loader = create_dataloader(lang_data[\"test\"], CONFIG)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZLUH9IMVND5"
      },
      "source": [
        "**Setup Optimizer:** Setup optimizer with different learning rates for seperate layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ydEXjnFHVSGI"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def setup_optimizer(model):\n",
        "    param_groups = []\n",
        "    # Check model layers and add appropiate learning rates\n",
        "    if hasattr(model, \"bert\"):\n",
        "        param_groups.append({\"params\" : model.bert.parameters(), \"lr\" : TrainConfig.BERT_LEARNING_RATE})\n",
        "    if hasattr(model, \"lstm\"):\n",
        "        param_groups.append({\"params\" : model.lstm.parameters(), \"lr\" : TrainConfig.LSTM_LEARNING_RATE})\n",
        "    if hasattr(model, \"crf\"):\n",
        "        param_groups.append({\"params\" : model.crf.parameters(), \"lr\" : TrainConfig.CRF_LEARNING_RATE})\n",
        "    optimizer = optim.Adam(param_groups, weight_decay = TrainConfig.WEIGHT_DECAY)\n",
        "\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxzyHtOLsuJ-"
      },
      "source": [
        "**Baseline:** Train baseline models; save weights and performance scores for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcqSTNMrXwIg",
        "outputId": "b4514c30-0861-407e-ebd6-e5d029b390a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 6/6 [04:31<00:00, 45.20s/it]\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "| language   |   train_f1 |   val_f1 |   test_f1 |\n",
              "|:-----------|-----------:|---------:|----------:|\n",
              "| mg         |   0.993728 | 0.933673 |  0.960352 |\n",
              "| fo         |   0.974224 | 0.897482 |  0.901099 |\n",
              "| co         |   0.956204 | 0.852308 |  0.81323  |\n",
              "| hsb        |   0.951443 | 0.923387 |  0.854578 |\n",
              "| bh         |   0.981549 | 0.888689 |  0.80212  |\n",
              "| cv         |   0.977741 | 0.892617 |  0.830443 |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "baseline_results = []\n",
        "\n",
        "# Iterate through low-resource languages\n",
        "for lang, lang_data in tqdm(low_resource_datasets.items(), ncols=80):\n",
        "\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(lang_data, TrainConfig)\n",
        "\n",
        "    # ------------------------------------------ TRAINING ------------------------------------------ #\n",
        "\n",
        "    model = BertBilstmCrf(TrainConfig.NUM_TAGS).to(TrainConfig.DEVICE)\n",
        "    optimizer = setup_optimizer(model)\n",
        "    best_model_state, train_f1, val_f1 = train_model(model, optimizer, train_loader, val_loader, TrainConfig)\n",
        "\n",
        "    # ------------------------------------------ EVALUATION ------------------------------------------ #\n",
        "    eval_model = BertBilstmCrf(TrainConfig.NUM_TAGS).to(TrainConfig.DEVICE)\n",
        "    eval_model.load_state_dict(best_model_state)\n",
        "    test_loss, test_f1 = evaluate_epoch(eval_model, test_loader, TrainConfig)\n",
        "\n",
        "    # ------------------------------------------ RESULTS ------------------------------------------ #\n",
        "    torch.save(best_model_state, f\"models/{lang}_baseline.pth\")\n",
        "\n",
        "    baseline_results.append({\n",
        "        \"language\" : lang,\n",
        "        \"train_f1\" : train_f1,\n",
        "        \"val_f1\"   : val_f1,\n",
        "        \"test_f1\"  : test_f1\n",
        "    })\n",
        "    \n",
        "# Save and display results\n",
        "baseline = pd.DataFrame(baseline_results)\n",
        "baseline.to_csv(\"results/baseline.csv\", index=False)\n",
        "\n",
        "markdown_table = baseline.to_markdown(index=False)\n",
        "display(Markdown(markdown_table))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline = pd.read_csv(\"results/baseline.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmv2uepyuddB"
      },
      "source": [
        "**Cross-lingual transfer learning:** Train model on high-resource language data, then fine-tune on target low-resource language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfVbQr1wudOo",
        "outputId": "666c8b2c-5412-4d0f-b1e2-02a7f6a8c5c3"
      },
      "outputs": [],
      "source": [
        "transfer_results = []\n",
        "\n",
        "for augmentation_factor in tqdm(range(1, 24), ncols=80):\n",
        "\n",
        "    high_resource_datasets = load_wikiann_datasets(TrainConfig.high_resource_langs, augmentation_factor * 240)\n",
        "\n",
        "    # Iterate through low-resource and adjacent high-resource languages\n",
        "    for (low_resource_lang, low_resource_data), (high_resource_lang, high_resource_data) in tqdm(zip(\n",
        "            low_resource_datasets.items(), high_resource_datasets.items()\n",
        "        ), ncols=80, leave=False):\n",
        "\n",
        "        high_train_loader, high_val_loader, _ = create_dataloaders(high_resource_data, TrainConfig)\n",
        "        low_train_loader, low_val_loader, low_test_loader = create_dataloaders(low_resource_data, TrainConfig)\n",
        "\n",
        "        # ------------------------------------------ PRE-TRAINING ------------------------------------------ #\n",
        "\n",
        "        high_resource_model = BertBilstmCrf(TrainConfig.NUM_TAGS).to(TrainConfig.DEVICE)\n",
        "        optimizer = setup_optimizer(high_resource_model)\n",
        "\n",
        "        high_resource_model_state, train_f1, val_f1 = train_model(high_resource_model, optimizer, high_train_loader, high_val_loader, TrainConfig)\n",
        "\n",
        "        # ------------------------------------------ FINE-TUNING ------------------------------------------ #\n",
        "\n",
        "        model = BertBilstmCrf(FineTuneConfig.NUM_TAGS).to(FineTuneConfig.DEVICE)\n",
        "        model.load_state_dict(high_resource_model_state)\n",
        "        optimizer = setup_optimizer(model)\n",
        "\n",
        "        best_model_state, train_f1, val_f1 = train_model(model, optimizer, low_train_loader, low_val_loader, FineTuneConfig)\n",
        "\n",
        "        # ------------------------------------------ EVALUATION ------------------------------------------ #\n",
        "\n",
        "        eval_model = BertBilstmCrf(FineTuneConfig.NUM_TAGS).to(FineTuneConfig.DEVICE)\n",
        "        eval_model.load_state_dict(best_model_state)\n",
        "        test_loss, test_f1 = evaluate_epoch(eval_model, low_test_loader, FineTuneConfig)\n",
        "\n",
        "        # ------------------------------------------ RESULTS ------------------------------------------ #\n",
        "        torch.save(best_model_state, f\"models/{low_resource_lang}_{high_resource_lang}_transfer.pth\")\n",
        "\n",
        "        baseline_performance = baseline.loc[baseline[\"language\"] == low_resource_lang, \"test_f1\"].item()\n",
        "        improvement = (test_f1 - baseline_performance) / baseline_performance * 100\n",
        "\n",
        "        transfer_results.append({\n",
        "            \"high_resource_language\" : high_resource_lang,\n",
        "            \"low_resource_lang\"      : low_resource_lang,\n",
        "            \"augmentation_factor\"    : augmentation_factor,\n",
        "            \"train_f1\"               : train_f1,\n",
        "            \"val_f1\"                 : val_f1,\n",
        "            \"test_f1\"                : test_f1,\n",
        "            \"improvement\"            : improvement\n",
        "        })\n",
        "\n",
        "        print(f\"Aug: {augmentation_factor}  {low_resource_lang} Improvement over baseline: {improvement:.5f}\")\n",
        "\n",
        "transfer_data = pd.DataFrame(transfer_results)\n",
        "transfer_data.to_csv(\"results/transfer_learning.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load unlabeled text data\n",
        "with open(\"data/unlabeled/bh_texts.txt\") as file:\n",
        "    unlabeled_sentences = file.readlines()\n",
        "\n",
        "unlabeled_data = pd.DataFrame({\"tokens\" : unlabeled_sentences})\n",
        "unlabeled_data[\"tokens\"] = unlabeled_data[\"tokens\"].apply(lambda sent: sent.split())\n",
        "unlabeled_data[\"ner_tags\"] = unlabeled_data[\"tokens\"].apply(lambda sent: [0] * len(sent))\n",
        "\n",
        "state = torch.load(\"models/bh_baseline.pth\")\n",
        "model = BertBilstmCrf(TrainConfig.NUM_TAGS).to(TrainConfig.DEVICE)\n",
        "model.load_state_dict(state)\n",
        "\n",
        "from pseudo_labeling import generate_pseudo_labels\n",
        "df = generate_pseudo_labels(model, unlabeled_data, TrainConfig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\lucas\\miniconda3\\envs\\multi-ner\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1458: RuntimeWarning: overflow encountered in cast\n",
            "  has_large_values = (abs_vals > 1e6).any()\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>ner_tags</th>\n",
              "      <th>confidence_score</th>\n",
              "      <th>entropy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[आर.एच., सॉन्डर्स, (सेंट, लॉरेंस, नदी), (968, ...</td>\n",
              "      <td>[1, 1, 1, 2, 0, 2, 2]</td>\n",
              "      <td>0.737924</td>\n",
              "      <td>0.772461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['Anders, Lindström']</td>\n",
              "      <td>[0, 1]</td>\n",
              "      <td>0.741672</td>\n",
              "      <td>0.800781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Karl, Ove, Knausgård, (, जनम, 1968, )]</td>\n",
              "      <td>[0, 0, 0, 1, 1, 1, 0]</td>\n",
              "      <td>0.700457</td>\n",
              "      <td>0.963379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[अटलांटिक, सिटी,, न्यू, जर्सी]</td>\n",
              "      <td>[5, 5, 5, 5]</td>\n",
              "      <td>0.758022</td>\n",
              "      <td>0.772949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[ओकरा, दोसरा, बियाह, से, बिटिया, रहल, Marie, d...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.973245</td>\n",
              "      <td>0.159058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>[गोरिल्लाज़, –, ``, डेयर, '']</td>\n",
              "      <td>[5, 5, 5, 5, 5]</td>\n",
              "      <td>0.803606</td>\n",
              "      <td>0.486328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>[इंग्लैंड, के, राजा, हेनरी, प्रथम]</td>\n",
              "      <td>[1, 1, 1, 1, 2]</td>\n",
              "      <td>0.773381</td>\n",
              "      <td>0.898438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>[''जोर, से, बोल'']</td>\n",
              "      <td>[0, 0, 0]</td>\n",
              "      <td>0.778720</td>\n",
              "      <td>0.427246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10000</th>\n",
              "      <td>[Hampton, Beach,, न्यू, हैम्पशायर]</td>\n",
              "      <td>[0, 1, 1, 0]</td>\n",
              "      <td>0.703552</td>\n",
              "      <td>1.029297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10001</th>\n",
              "      <td>[एफ.सी., रोई, हेशबोन, तेल, अवीव]</td>\n",
              "      <td>[3, 3, 4, 4, 4]</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.548828</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10002 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  tokens  \\\n",
              "0      [आर.एच., सॉन्डर्स, (सेंट, लॉरेंस, नदी), (968, ...   \n",
              "1                                  ['Anders, Lindström']   \n",
              "2                [Karl, Ove, Knausgård, (, जनम, 1968, )]   \n",
              "3                         [अटलांटिक, सिटी,, न्यू, जर्सी]   \n",
              "4      [ओकरा, दोसरा, बियाह, से, बिटिया, रहल, Marie, d...   \n",
              "...                                                  ...   \n",
              "9997                       [गोरिल्लाज़, –, ``, डेयर, '']   \n",
              "9998                  [इंग्लैंड, के, राजा, हेनरी, प्रथम]   \n",
              "9999                                  [''जोर, से, बोल'']   \n",
              "10000                 [Hampton, Beach,, न्यू, हैम्पशायर]   \n",
              "10001                   [एफ.सी., रोई, हेशबोन, तेल, अवीव]   \n",
              "\n",
              "                                                ner_tags  confidence_score  \\\n",
              "0                                  [1, 1, 1, 2, 0, 2, 2]          0.737924   \n",
              "1                                                 [0, 1]          0.741672   \n",
              "2                                  [0, 0, 0, 1, 1, 1, 0]          0.700457   \n",
              "3                                           [5, 5, 5, 5]          0.758022   \n",
              "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          0.973245   \n",
              "...                                                  ...               ...   \n",
              "9997                                     [5, 5, 5, 5, 5]          0.803606   \n",
              "9998                                     [1, 1, 1, 1, 2]          0.773381   \n",
              "9999                                           [0, 0, 0]          0.778720   \n",
              "10000                                       [0, 1, 1, 0]          0.703552   \n",
              "10001                                    [3, 3, 4, 4, 4]          0.868421   \n",
              "\n",
              "        entropy  \n",
              "0      0.772461  \n",
              "1      0.800781  \n",
              "2      0.963379  \n",
              "3      0.772949  \n",
              "4      0.159058  \n",
              "...         ...  \n",
              "9997   0.486328  \n",
              "9998   0.898438  \n",
              "9999   0.427246  \n",
              "10000  1.029297  \n",
              "10001  0.548828  \n",
              "\n",
              "[10002 rows x 4 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iterative_pseudo_labeling_results = []\n",
        "\n",
        "# Iterate through low-resource languages\n",
        "high_resource_datasets = load_wikiann_datasets(TrainConfig.high_resource_langs, 10000)\n",
        "\n",
        "# Iterate through low-resource and adjacent high-resource languages\n",
        "for (lang, low_resource_data), (_, high_resource_data) in tqdm(zip(\n",
        "        low_resource_datasets.items(), high_resource_datasets.items()\n",
        "    ), ncols=80, leave=False):\n",
        "\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(low_resource_data, TrainConfig)\n",
        "    unlabeled_data = high_resource_data[\"train\"]\n",
        "\n",
        "    # ------------------------------------------ TRAINING ------------------------------------------ #\n",
        "\n",
        "    model = BertBilstmCrf(TrainConfig.NUM_TAGS).to(TrainConfig.DEVICE)\n",
        "    model.load_state_dict(torch.load(f\"models/{lang}_baseline.pth\"))\n",
        "    optimizer = setup_optimizer(model)\n",
        "    best_model_state, train_f1, val_f1 = train_pseudo_labeling(model, optimizer, train_loader, val_loader, unlabeled_data, PseudoLabelingConfig)\n",
        "\n",
        "    # ------------------------------------------ EVALUATION ------------------------------------------ #\n",
        "    \n",
        "    eval_model = BertBilstmCrf(TrainConfig.NUM_TAGS).to(TrainConfig.DEVICE)\n",
        "    eval_model.load_state_dict(best_model_state)\n",
        "    test_loss, test_f1 = evaluate_epoch(eval_model, test_loader, TrainConfig)\n",
        "\n",
        "    # ------------------------------------------ RESULTS ------------------------------------------ #\n",
        "    torch.save(best_model_state, f\"models/{lang}_iterative_pseudo_labeling.pth\")\n",
        "\n",
        "    baseline_performance = baseline.loc[baseline[\"language\"] == lang, \"test_f1\"].item()\n",
        "    improvement = (test_f1 - baseline_performance) / baseline_performance * 100\n",
        "\n",
        "    iterative_pseudo_labeling_results.append({\n",
        "        \"language\"    : lang,\n",
        "        \"train_f1\"    : train_f1,\n",
        "        \"val_f1\"      : val_f1,\n",
        "        \"test_f1\"     : test_f1,\n",
        "        \"improvement\" : improvement\n",
        "    })\n",
        "\n",
        "    print(f\"Language: {lang}    Improvement over baseline: {improvement:.5f}\")\n",
        "\n",
        "\n",
        "# Save results\n",
        "iterative_pseudo_labeling = pd.DataFrame(iterative_pseudo_labeling_results)\n",
        "iterative_pseudo_labeling.to_csv(\"results/iterative_pseudo_labeling.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Added 279 rows of data\n",
            "Added 10 rows of data\n",
            "Added 126 rows of data\n",
            "Added 0 rows of data\n",
            "Added 79 rows of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [07:48, 468.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: mg    Improvement over baseline: 0.00000\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Added 71 rows of data\n",
            "Added 3 rows of data\n",
            "Added 36 rows of data\n",
            "Added 121 rows of data\n",
            "Added 280 rows of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2it [15:29, 464.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: fo    Improvement over baseline: -1.74216\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Added 0 rows of data\n",
            "Added 123 rows of data\n",
            "Added 0 rows of data\n",
            "Added 6 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 66 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 2 rows of data\n",
            "Added 1 rows of data\n",
            "Added 3 rows of data\n",
            "Added 4 rows of data\n",
            "Added 4 rows of data\n",
            "Added 4 rows of data\n",
            "Added 4 rows of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3it [29:21, 632.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: co    Improvement over baseline: 7.65550\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4it [41:23, 667.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: hsb    Improvement over baseline: -0.63025\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Added 110 rows of data\n",
            "Added 196 rows of data\n",
            "Added 195 rows of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [45:35, 517.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: bh    Improvement over baseline: 3.96476\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: cv    Improvement over baseline: -3.24826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "iterative_pseudo_labeling_results = []\n",
        "\n",
        "# Iterate through low-resource languages\n",
        "high_resource_datasets = load_wikiann_datasets(TrainConfig.high_resource_langs, 10000)\n",
        "\n",
        "# Iterate through low-resource and adjacent high-resource languages\n",
        "for (lang, low_resource_data), (_, high_resource_data) in tqdm(zip(\n",
        "        low_resource_datasets.items(), high_resource_datasets.items()\n",
        "    ), ncols=80, leave=False):\n",
        "\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(low_resource_data, TrainConfig)\n",
        "    unlabeled_data = high_resource_data[\"train\"]\n",
        "\n",
        "    # ------------------------------------------ TRAINING ------------------------------------------ #\n",
        "\n",
        "    model = BertBilstmCrf(TrainConfig.NUM_TAGS).to(TrainConfig.DEVICE)\n",
        "    optimizer = setup_optimizer(model)\n",
        "    best_model_state, train_f1, val_f1 = train_pseudo_labeling(model, optimizer, train_loader, val_loader, unlabeled_data, PseudoLabelingConfig)\n",
        "\n",
        "    # ------------------------------------------ EVALUATION ------------------------------------------ #\n",
        "    \n",
        "    eval_model = BertBilstmCrf(TrainConfig.NUM_TAGS).to(TrainConfig.DEVICE)\n",
        "    eval_model.load_state_dict(best_model_state)\n",
        "    test_loss, test_f1 = evaluate_epoch(eval_model, test_loader, TrainConfig)\n",
        "\n",
        "    # ------------------------------------------ RESULTS ------------------------------------------ #\n",
        "    torch.save(best_model_state, f\"models/{lang}_iterative_pseudo_labeling.pth\")\n",
        "\n",
        "    baseline_performance = baseline.loc[baseline[\"language\"] == lang, \"test_f1\"].item()\n",
        "    improvement = (test_f1 - baseline_performance) / baseline_performance * 100\n",
        "\n",
        "    iterative_pseudo_labeling_results.append({\n",
        "        \"language\"    : lang,\n",
        "        \"train_f1\"    : train_f1,\n",
        "        \"val_f1\"      : val_f1,\n",
        "        \"test_f1\"     : test_f1,\n",
        "        \"improvement\" : improvement\n",
        "    })\n",
        "\n",
        "    print(f\"Language: {lang}    Improvement over baseline: {improvement:.5f}\")\n",
        "\n",
        "\n",
        "# Save results\n",
        "iterative_pseudo_labeling = pd.DataFrame(iterative_pseudo_labeling_results)\n",
        "iterative_pseudo_labeling.to_csv(\"results/iterative_pseudo_labeling.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Iterative Pseudo Labeling:** During the training process, continously generate labels for unlabeled data and add high-confidence pseudo-labels to original training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 324 rows of data\n",
            "Added 0 rows of data\n",
            "Language: mg    Improvement over baseline: -1.37615\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 41 rows of data\n",
            "Added 72 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Language: fo    Improvement over baseline: -5.57491\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 21 rows of data\n",
            "Added 5 rows of data\n",
            "Added 0 rows of data\n",
            "Added 2 rows of data\n",
            "Added 0 rows of data\n",
            "Added 85 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Language: co    Improvement over baseline: -8.61244\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 2 rows of data\n",
            "Added 21 rows of data\n",
            "Added 5 rows of data\n",
            "Added 9 rows of data\n",
            "Added 8 rows of data\n",
            "Added 26 rows of data\n",
            "Added 0 rows of data\n",
            "Added 3 rows of data\n",
            "Added 10 rows of data\n",
            "Added 11 rows of data\n",
            "Added 4 rows of data\n",
            "Added 3 rows of data\n",
            "Added 2 rows of data\n",
            "Language: hsb    Improvement over baseline: -7.77311\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n",
            "Added 1 rows of data\n",
            "Added 2 rows of data\n",
            "Added 8 rows of data\n",
            "Added 3 rows of data\n",
            "Added 10 rows of data\n",
            "Added 7 rows of data\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m BertBilstmCrf(TrainConfig\u001b[38;5;241m.\u001b[39mNUM_TAGS)\u001b[38;5;241m.\u001b[39mto(TrainConfig\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[0;32m     19\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m setup_optimizer(model)\n\u001b[1;32m---> 20\u001b[0m best_model_state, train_f1, val_f1 \u001b[38;5;241m=\u001b[39m train_pseudo_labeling(model, optimizer, train_loader, val_loader, unlabeled_data, PseudoLabelingConfig)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# ------------------------------------------ EVALUATION ------------------------------------------ #\u001b[39;00m\n\u001b[0;32m     24\u001b[0m eval_model \u001b[38;5;241m=\u001b[39m BertBilstmCrf(TrainConfig\u001b[38;5;241m.\u001b[39mNUM_TAGS)\u001b[38;5;241m.\u001b[39mto(TrainConfig\u001b[38;5;241m.\u001b[39mDEVICE)\n",
            "File \u001b[1;32mc:\\Users\\lucas\\Desktop\\NLP_Research\\MULTI-NER\\pseudo_labeling.py:51\u001b[0m, in \u001b[0;36mtrain_pseudo_labeling\u001b[1;34m(model, optimizer, train_loader, val_loader, unlabeled_data, CONFIG)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Stop training if model doesn't improve\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Generate pseudo-labels with trained model on unlabeled data\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m pseudo_labels \u001b[38;5;241m=\u001b[39m generate_pseudo_labels(model, unlabeled_data, CONFIG)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_tags\u001b[39m(row):\n\u001b[0;32m     54\u001b[0m     high_confidence \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m CONFIG\u001b[38;5;241m.\u001b[39mCONFIDENCE_THRESHOLD\n",
            "File \u001b[1;32mc:\\Users\\lucas\\Desktop\\NLP_Research\\MULTI-NER\\pseudo_labeling.py:99\u001b[0m, in \u001b[0;36mgenerate_pseudo_labels\u001b[1;34m(model, unlabeled_data, CONFIG)\u001b[0m\n\u001b[0;32m     96\u001b[0m batch \u001b[38;5;241m=\u001b[39m {key : value\u001b[38;5;241m.\u001b[39mto(CONFIG\u001b[38;5;241m.\u001b[39mDEVICE) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 99\u001b[0m     emissions, _ \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m    100\u001b[0m     predicted_tags \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode(emissions, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# Compute sequence probabilities and entropy\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\lucas\\miniconda3\\envs\\multi-ner\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\lucas\\miniconda3\\envs\\multi-ner\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\lucas\\miniconda3\\envs\\multi-ner\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\lucas\\Desktop\\NLP_Research\\MULTI-NER\\model.py:31\u001b[0m, in \u001b[0;36mBertBilstmCrf.forward\u001b[1;34m(self, input_ids, target_tags, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     28\u001b[0m lstm_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(sequence_output)\n\u001b[0;32m     29\u001b[0m emissions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(lstm_output)\n\u001b[1;32m---> 31\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrf(emissions, target_tags, mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mbool(), reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m emissions, loss\n",
            "File \u001b[1;32mc:\\Users\\lucas\\miniconda3\\envs\\multi-ner\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\lucas\\miniconda3\\envs\\multi-ner\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\lucas\\miniconda3\\envs\\multi-ner\\Lib\\site-packages\\torchcrf\\__init__.py:97\u001b[0m, in \u001b[0;36mCRF.forward\u001b[1;34m(self, emissions, tags, mask, reduction)\u001b[0m\n\u001b[0;32m     94\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(tags, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first:\n\u001b[1;32m---> 97\u001b[0m     emissions \u001b[38;5;241m=\u001b[39m emissions\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     98\u001b[0m     tags \u001b[38;5;241m=\u001b[39m tags\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     99\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "iterative_pseudo_labeling_results = []\n",
        "\n",
        "# Iterate through low-resource languages\n",
        "for lang, lang_data in low_resource_datasets.items():\n",
        "\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(lang_data, PseudoLabelingConfig)\n",
        "\n",
        "    # Load unlabeled text data\n",
        "    with open(f\"data/unlabeled/{lang}_texts.txt\") as file:\n",
        "        unlabeled_sentences = file.readlines()\n",
        "\n",
        "    unlabeled_data = pd.DataFrame({\"tokens\" : unlabeled_sentences})\n",
        "    unlabeled_data[\"tokens\"] = unlabeled_data[\"tokens\"].apply(lambda sent: sent.split())\n",
        "    unlabeled_data[\"ner_tags\"] = unlabeled_data[\"tokens\"].apply(lambda sent: [0] * len(sent))\n",
        "\n",
        "    # ------------------------------------------ TRAINING ------------------------------------------ #\n",
        "\n",
        "    model = BertBilstmCrf(TrainConfig.NUM_TAGS).to(TrainConfig.DEVICE)\n",
        "    optimizer = setup_optimizer(model)\n",
        "    best_model_state, train_f1, val_f1 = train_pseudo_labeling(model, optimizer, train_loader, val_loader, unlabeled_data, PseudoLabelingConfig)\n",
        "\n",
        "    # ------------------------------------------ EVALUATION ------------------------------------------ #\n",
        "    \n",
        "    eval_model = BertBilstmCrf(TrainConfig.NUM_TAGS).to(TrainConfig.DEVICE)\n",
        "    eval_model.load_state_dict(best_model_state)\n",
        "    test_loss, test_f1 = evaluate_epoch(eval_model, test_loader, TrainConfig)\n",
        "\n",
        "    # ------------------------------------------ RESULTS ------------------------------------------ #\n",
        "    torch.save(best_model_state, f\"models/{lang}_iterative_pseudo_labeling.pth\")\n",
        "\n",
        "    baseline_performance = baseline.loc[baseline[\"language\"] == lang, \"test_f1\"].item()\n",
        "    improvement = (test_f1 - baseline_performance) / baseline_performance * 100\n",
        "\n",
        "    iterative_pseudo_labeling_results.append({\n",
        "        \"language\"    : lang,\n",
        "        \"train_f1\"    : train_f1,\n",
        "        \"val_f1\"      : val_f1,\n",
        "        \"test_f1\"     : test_f1,\n",
        "        \"improvement\" : improvement\n",
        "    })\n",
        "\n",
        "    print(f\"Language: {lang}    Improvement over baseline: {improvement:.5f}\")\n",
        "\n",
        "\n",
        "# Save results\n",
        "iterative_pseudo_labeling = pd.DataFrame(iterative_pseudo_labeling_results)\n",
        "iterative_pseudo_labeling.to_csv(\"results/iterative_pseudo_labeling.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "multi-ner",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
