{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEKDEcZY0Ed9"
      },
      "source": [
        "# **Machine learning for low-resource NLP**: Advancing AI for Linguistic Inclusion\n",
        "Cross-lingual transfer learning and pseudo-labeling for multilingual named entity recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMkZmRoX2xLR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model import BertBilstmCrf\n",
        "from dataloader import create_dataloaders\n",
        "from training import train_model, train_pseudo_labeling, evaluate_epoch\n",
        "from config import BaseConfig, TrainConfig, FineTuneConfig, PseudoLabelingConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KsqUJuTgpSX"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(BaseConfig.RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "83jA6h2-zZiy",
        "outputId": "c5b149b6-ec47-41da-ee95-c5980771a869"
      },
      "outputs": [],
      "source": [
        "def load_wikiann_datasets(language_codes, cutoff=None):\n",
        "\n",
        "    language_data = {}\n",
        "    for lang in language_codes:\n",
        "\n",
        "        # Load raw data from hugging face\n",
        "        lang_dataset = load_dataset(\"unimelb-nlp/wikiann\", name=lang)\n",
        "\n",
        "        # Get data from different splits and combine\n",
        "        train_df = pd.DataFrame(lang_dataset[\"train\"])\n",
        "        val_df = pd.DataFrame(lang_dataset[\"validation\"])\n",
        "        test_df = pd.DataFrame(lang_dataset[\"test\"])\n",
        "\n",
        "        complete_df = pd.concat([train_df, val_df, test_df]).reset_index(drop=True)\n",
        "        complete_df = complete_df.head(cutoff) if cutoff else complete_df\n",
        "\n",
        "        # Split data into new train/val/test splits\n",
        "        train, temp = train_test_split(\n",
        "            complete_df, test_size=0.2, random_state=BaseConfig.RANDOM_STATE\n",
        "        )\n",
        "        val, test = train_test_split(\n",
        "            temp, test_size=0.5, random_state=BaseConfig.RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        language_data[lang] = {\"train\": train, \"val\": val, \"test\": test}\n",
        "\n",
        "    return language_data\n",
        "\n",
        "\n",
        "# Download and store data\n",
        "low_resource_datasets = load_wikiann_datasets(BaseConfig.low_resource_langs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_optimizer(model, CONFIG):\n",
        "    param_groups = []\n",
        "    # Check model layers and add appropiate learning rates\n",
        "    if hasattr(model, \"bert\"):\n",
        "        param_groups.append(\n",
        "            {\"params\": model.bert.parameters(), \"lr\": CONFIG.BERT_LEARNING_RATE}\n",
        "        )\n",
        "    if hasattr(model, \"lstm\"):\n",
        "        param_groups.append(\n",
        "            {\"params\": model.lstm.parameters(), \"lr\": CONFIG.LSTM_LEARNING_RATE}\n",
        "        )\n",
        "    if hasattr(model, \"crf\"):\n",
        "        param_groups.append(\n",
        "            {\"params\": model.crf.parameters(), \"lr\": CONFIG.CRF_LEARNING_RATE}\n",
        "        )\n",
        "    optimizer = optim.Adam(param_groups, weight_decay=CONFIG.WEIGHT_DECAY)\n",
        "\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline Experiment\n",
        "Baseline BERT-BiLSTM-CRF model trained on multilingual NER data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcqSTNMrXwIg",
        "outputId": "b4514c30-0861-407e-ebd6-e5d029b390a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 6/6 [04:31<00:00, 45.20s/it]\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "| language   |   train_f1 |   val_f1 |   test_f1 |\n",
              "|:-----------|-----------:|---------:|----------:|\n",
              "| mg         |   0.993728 | 0.933673 |  0.960352 |\n",
              "| fo         |   0.974224 | 0.897482 |  0.901099 |\n",
              "| co         |   0.956204 | 0.852308 |  0.81323  |\n",
              "| hsb        |   0.951443 | 0.923387 |  0.854578 |\n",
              "| bh         |   0.981549 | 0.888689 |  0.80212  |\n",
              "| cv         |   0.977741 | 0.892617 |  0.830443 |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "baseline_results = []\n",
        "\n",
        "# Iterate through low-resource languages\n",
        "for lang, lang_data in tqdm(low_resource_datasets.items(), ncols=80):\n",
        "\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(lang_data)\n",
        "\n",
        "    # ------------------------------------------ TRAINING ------------------------------------------ #\n",
        "\n",
        "    model = BertBilstmCrf(BaseConfig.NUM_TAGS).to(BaseConfig.DEVICE)\n",
        "    optimizer = setup_optimizer(model, TrainConfig)\n",
        "    best_model_state, train_f1, val_f1 = train_model(\n",
        "        model, optimizer, train_loader, val_loader, TrainConfig\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------ EVALUATION ------------------------------------------ #\n",
        "    eval_model = BertBilstmCrf(BaseConfig.NUM_TAGS).to(BaseConfig.DEVICE)\n",
        "    eval_model.load_state_dict(best_model_state, TrainConfig)\n",
        "    test_loss, test_f1 = evaluate_epoch(eval_model, test_loader, TrainConfig)\n",
        "\n",
        "    # ------------------------------------------ RESULTS ------------------------------------------ #\n",
        "    torch.save(best_model_state, f\"models/{lang}_baseline.pth\")\n",
        "\n",
        "    baseline_results.append(\n",
        "        {\"language\": lang, \"train_f1\": train_f1, \"val_f1\": val_f1, \"test_f1\": test_f1}\n",
        "    )\n",
        "\n",
        "# Save and display results\n",
        "baseline = pd.DataFrame(baseline_results)\n",
        "baseline.to_csv(\"results/baseline.csv\", index=False)\n",
        "\n",
        "markdown_table = baseline.to_markdown(index=False)\n",
        "display(Markdown(markdown_table))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline = pd.read_csv(\"results/baseline.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-lingual Transfer Learning\n",
        "A technique where a model trained on one language (usually with more labeled data) is adapted to perform well on another language, leveraging shared linguistic representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfVbQr1wudOo",
        "outputId": "666c8b2c-5412-4d0f-b1e2-02a7f6a8c5c3"
      },
      "outputs": [],
      "source": [
        "transfer_results = []\n",
        "\n",
        "for augmentation_factor in tqdm(range(1, 24), ncols=80):\n",
        "\n",
        "    high_resource_datasets = load_wikiann_datasets(\n",
        "        BaseConfig.high_resource_langs, augmentation_factor * 240\n",
        "    )\n",
        "\n",
        "    # Iterate through low-resource and adjacent high-resource languages\n",
        "    for (low_resource_lang, low_resource_data), (\n",
        "        high_resource_lang,\n",
        "        high_resource_data,\n",
        "    ) in tqdm(\n",
        "        zip(low_resource_datasets.items(), high_resource_datasets.items()),\n",
        "        ncols=80,\n",
        "        leave=False,\n",
        "    ):\n",
        "\n",
        "        high_train_loader, high_val_loader, _ = create_dataloaders(high_resource_data)\n",
        "        low_train_loader, low_val_loader, low_test_loader = create_dataloaders(\n",
        "            low_resource_data\n",
        "        )\n",
        "\n",
        "        # ------------------------------------------ PRE-TRAINING ------------------------------------------ #\n",
        "\n",
        "        high_resource_model = BertBilstmCrf(BaseConfig.NUM_TAGS).to(BaseConfig.DEVICE)\n",
        "        optimizer = setup_optimizer(high_resource_model, TrainConfig)\n",
        "\n",
        "        high_resource_model_state, train_f1, val_f1 = train_model(\n",
        "            high_resource_model,\n",
        "            optimizer,\n",
        "            high_train_loader,\n",
        "            high_val_loader,\n",
        "            TrainConfig,\n",
        "        )\n",
        "\n",
        "        # ------------------------------------------ FINE-TUNING ------------------------------------------ #\n",
        "\n",
        "        model = BertBilstmCrf(BaseConfig.NUM_TAGS).to(BaseConfig.DEVICE)\n",
        "        model.load_state_dict(high_resource_model_state)\n",
        "        optimizer = setup_optimizer(model, FineTuneConfig)\n",
        "\n",
        "        best_model_state, train_f1, val_f1 = train_model(\n",
        "            model, optimizer, low_train_loader, low_val_loader, FineTuneConfig\n",
        "        )\n",
        "\n",
        "        # ------------------------------------------ EVALUATION ------------------------------------------ #\n",
        "\n",
        "        eval_model = BertBilstmCrf(BaseConfig.NUM_TAGS).to(BaseConfig.DEVICE)\n",
        "        eval_model.load_state_dict(best_model_state)\n",
        "        test_loss, test_f1 = evaluate_epoch(eval_model, low_test_loader)\n",
        "\n",
        "        # ------------------------------------------ RESULTS ------------------------------------------ #\n",
        "        torch.save(\n",
        "            best_model_state,\n",
        "            f\"models/{low_resource_lang}_{high_resource_lang}_transfer.pth\",\n",
        "        )\n",
        "\n",
        "        baseline_performance = baseline.loc[\n",
        "            baseline[\"language\"] == low_resource_lang, \"test_f1\"\n",
        "        ].item()\n",
        "        improvement = (test_f1 - baseline_performance) / baseline_performance * 100\n",
        "\n",
        "        transfer_results.append(\n",
        "            {\n",
        "                \"high_resource_language\": high_resource_lang,\n",
        "                \"low_resource_lang\": low_resource_lang,\n",
        "                \"augmentation_factor\": augmentation_factor,\n",
        "                \"train_f1\": train_f1,\n",
        "                \"val_f1\": val_f1,\n",
        "                \"test_f1\": test_f1,\n",
        "                \"improvement\": improvement,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"Aug: {augmentation_factor}  {low_resource_lang} Improvement over baseline: {improvement:.5f}\"\n",
        "        )\n",
        "\n",
        "transfer_data = pd.DataFrame(transfer_results)\n",
        "transfer_data.to_csv(\"results/transfer_learning.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Iterative Pseudo Labeling\n",
        "A semi-supervised learning approach where a model generates predictions on unlabeled data, selects confident predictions as pseudo-labels, and retrains iteratively to improve performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [04:47, 287.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: mg    Improvement over baseline: 0.45872\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Added 293 rows of data\n",
            "Added 0 rows of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2it [10:51, 332.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: fo    Improvement over baseline: 2.78746\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3it [14:29, 280.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: co    Improvement over baseline: 2.39234\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Added 13 rows of data\n",
            "Added 2 rows of data\n",
            "Added 0 rows of data\n",
            "Added 1 rows of data\n",
            "Added 0 rows of data\n",
            "Added 2 rows of data\n",
            "Added 0 rows of data\n",
            "Added 0 rows of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4it [23:43, 388.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: hsb    Improvement over baseline: 0.21008\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Added 206 rows of data\n",
            "Added 0 rows of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [28:08, 344.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: bh    Improvement over baseline: 2.86344\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n",
            "Early epoch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: cv    Improvement over baseline: 1.62413\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "iterative_pseudo_labeling_results = []\n",
        "\n",
        "# Iterate through low-resource languages\n",
        "high_resource_datasets = load_wikiann_datasets(BaseConfig.high_resource_langs, 10000)\n",
        "\n",
        "# Iterate through low-resource and adjacent high-resource languages\n",
        "for (lang, low_resource_data), (_, high_resource_data) in tqdm(\n",
        "    zip(low_resource_datasets.items(), high_resource_datasets.items()),\n",
        "    ncols=80,\n",
        "    leave=False,\n",
        "):\n",
        "\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(low_resource_data)\n",
        "    unlabeled_data = high_resource_data[\"train\"]\n",
        "\n",
        "    # ------------------------------------------ TRAINING ------------------------------------------ #\n",
        "\n",
        "    model = BertBilstmCrf(BaseConfig.NUM_TAGS).to(BaseConfig.DEVICE)\n",
        "    model.load_state_dict(torch.load(f\"models/{lang}_baseline.pth\"))\n",
        "    optimizer = setup_optimizer(model, PseudoLabelingConfig)\n",
        "    best_model_state, train_f1, val_f1 = train_pseudo_labeling(\n",
        "        model, optimizer, train_loader, val_loader, unlabeled_data, PseudoLabelingConfig\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------ EVALUATION ------------------------------------------ #\n",
        "\n",
        "    eval_model = BertBilstmCrf(BaseConfig.NUM_TAGS).to(BaseConfig.DEVICE)\n",
        "    eval_model.load_state_dict(best_model_state)\n",
        "    test_loss, test_f1 = evaluate_epoch(eval_model, test_loader)\n",
        "\n",
        "    # ------------------------------------------ RESULTS ------------------------------------------ #\n",
        "    torch.save(best_model_state, f\"models/{lang}_iterative_pseudo_labeling.pth\")\n",
        "\n",
        "    baseline_performance = baseline.loc[baseline[\"language\"] == lang, \"test_f1\"].item()\n",
        "    improvement = (test_f1 - baseline_performance) / baseline_performance * 100\n",
        "\n",
        "    iterative_pseudo_labeling_results.append(\n",
        "        {\n",
        "            \"language\": lang,\n",
        "            \"train_f1\": train_f1,\n",
        "            \"val_f1\": val_f1,\n",
        "            \"test_f1\": test_f1,\n",
        "            \"improvement\": improvement,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"Language: {lang}    Improvement over baseline: {improvement:.5f}\")\n",
        "\n",
        "# Save results\n",
        "iterative_pseudo_labeling = pd.DataFrame(iterative_pseudo_labeling_results)\n",
        "iterative_pseudo_labeling.to_csv(\"results/18iterative_pseudo_labeling.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "9:\n",
        "\n",
        "class PseudoLabelingConfig(BaseConfig):\n",
        "    EPOCHS                = 25\n",
        "    PATIENCE              = 5\n",
        "    BERT_LEARNING_RATE    = 0.00002\n",
        "    LSTM_LEARNING_RATE    = 0.003\n",
        "    CRF_LEARNING_RATE     = 0.00003\n",
        "\n",
        "    CONFIDENCE_QUANTILE   = 0.95\n",
        "    PSEUDO_DELAY          = 8\n",
        "    ENTROPY_THRESHOLD     = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "10:\n",
        "\n",
        "class PseudoLabelingConfig(BaseConfig):\n",
        "    EPOCHS                = 25\n",
        "    PATIENCE              = 5\n",
        "    BERT_LEARNING_RATE    = 0.00002\n",
        "    LSTM_LEARNING_RATE    = 0.003\n",
        "    CRF_LEARNING_RATE     = 0.00003\n",
        "\n",
        "    CONFIDENCE_QUANTILE   = 0.90\n",
        "    PSEUDO_DELAY          = 8\n",
        "    ENTROPY_THRESHOLD     = 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "multi-ner",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
